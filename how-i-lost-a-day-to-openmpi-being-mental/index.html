<!doctype html><html lang=en-GB><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://jamiejquinn.com/static/favicon.ico><title>How I lost a Day to OpenMPI Being Mental | Jamie J Quinn</title>
<meta name=title content="How I lost a Day to OpenMPI Being Mental"><meta name=description content="So at Glasgow Uni we have this little cluster for the maths department which happens to including about ten machines set up to work with torque (a job scheduling system). I discovered that these machines hadn&rsquo;t had anything run on them for literally months, what a waste of resources! To rectify this atrocity I decided to try and run my MPI enabled code on all ten machines.
Problem One Turns out two of the machines have eight cores and the other eight have twelve cores."><meta name=keywords content="openmpi,code,hpc,mpi,fortran,"><meta property="og:url" content="https://jamiejquinn.com/how-i-lost-a-day-to-openmpi-being-mental/"><meta property="og:site_name" content="Jamie J Quinn"><meta property="og:title" content="How I lost a Day to OpenMPI Being Mental"><meta property="og:description" content="So at Glasgow Uni we have this little cluster for the maths department which happens to including about ten machines set up to work with torque (a job scheduling system). I discovered that these machines hadn&amp;rsquo;t had anything run on them for literally months, what a waste of resources! To rectify this atrocity I decided to try and run my MPI enabled code on all ten machines.
Problem One Turns out two of the machines have eight cores and the other eight have twelve cores."><meta property="og:locale" content="en-GB"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2017-05-19T13:28:58+01:00"><meta property="article:modified_time" content="2017-05-19T13:28:58+01:00"><meta property="article:tag" content="Openmpi"><meta property="article:tag" content="Code"><meta property="article:tag" content="Hpc"><meta property="article:tag" content="Mpi"><meta property="article:tag" content="Fortran"><meta property="og:image" content="https://jamiejquinn.com/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jamiejquinn.com/images/share.png"><meta name=twitter:title content="How I lost a Day to OpenMPI Being Mental"><meta name=twitter:description content="So at Glasgow Uni we have this little cluster for the maths department which happens to including about ten machines set up to work with torque (a job scheduling system). I discovered that these machines hadn&rsquo;t had anything run on them for literally months, what a waste of resources! To rectify this atrocity I decided to try and run my MPI enabled code on all ten machines.
Problem One Turns out two of the machines have eight cores and the other eight have twelve cores."><meta itemprop=name content="How I lost a Day to OpenMPI Being Mental"><meta itemprop=description content="So at Glasgow Uni we have this little cluster for the maths department which happens to including about ten machines set up to work with torque (a job scheduling system). I discovered that these machines hadn&rsquo;t had anything run on them for literally months, what a waste of resources! To rectify this atrocity I decided to try and run my MPI enabled code on all ten machines.
Problem One Turns out two of the machines have eight cores and the other eight have twelve cores."><meta itemprop=datePublished content="2017-05-19T13:28:58+01:00"><meta itemprop=dateModified content="2017-05-19T13:28:58+01:00"><meta itemprop=wordCount content="391"><meta itemprop=image content="https://jamiejquinn.com/images/share.png"><meta itemprop=keywords content="Openmpi,Code,Hpc,Mpi,Fortran"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{text-decoration:none;color:#c50}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%;display:block;margin-left:auto;margin-right:auto}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#3273dc}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]]}}</script></head><body><header><a href=/ class=title><h2>Jamie J Quinn / How I lost a Day to OpenMPI Being Mental</h2></a><nav><a href=/>Home</a>
<a href=/blog/>Blog</a>
<a href=/presentations/>Presentations</a>
<a href=/projects/>Projects</a>
<a href=/publications/>Publications</a>
<a href=/tunes/>Tunes</a>
<a href=http://art.jamiejquinn.com>Art</a>
<a href=/jamie_quinn_cv.pdf>CV</a></nav></header><main><h1>How I lost a Day to OpenMPI Being Mental</h1><p><i><time datetime=2017-05-19 pubdate>19 May, 2017</time></i></p><content><p>So at Glasgow Uni we have this little cluster for the maths department which happens to including about ten machines set up to work with torque (a job scheduling system). I discovered that these machines hadn&rsquo;t had <em>anything</em> run on them for literally months, what a waste of resources! To rectify this atrocity I decided to try and run my MPI enabled code on <em>all ten machines</em>.</p><h3 id=problem-one>Problem One</h3><p>Turns out two of the machines have eight cores and the other eight have twelve cores. I can&rsquo;t mix and match with core number (AFAIK) so I guess I&rsquo;ll just run the code on <em>all eight machines</em>.</p><h3 id=problem-two>Problem Two</h3><p>The vendor supplied fortran compiler and openmpi library are pretty old and pretty useless. They produce binaries that run about 3x slower than current versions so I guess I&rsquo;ll have to build the latest versions myself.</p><h3 id=problem-three>Problem Three</h3><p>I run the code on a single machine. Works great, not so great performance but I can perhaps sort that out later. I run the code on multiple machines and <strong>instant</strong> crash.</p><pre tabindex=0><code>ERROR: received unexpected process identifier
</code></pre><p>Google doesn&rsquo;t help, the only result is someone trying to run openmpi on one machine with multiple IP addresses on a single interface. So I spend <em>six hours</em> looking through the verbose MPI output to discover that upon MPI setting itself up, each MPI process contacts every other process.</p><p>Contacting another process on the same machine? Absolutely fine, nae bother.</p><p>Contacting another machine over the network? Not fine, definitely bother.</p><p>Why? It was contacting the other machines through the interface <code>virbr0</code> which happens to have the <em>same IP address</em> on <em>every single machine</em>. So the conversation between a machine <em>and itself</em> was going something like this:</p><p>Machine A: Hey, 192.168.1.1, you&rsquo;re running this MPI thing?\
Machine A: Do I know you?\
Machine A: Apparently not.\
Machine A: Well, let&rsquo;s decide to produce a very confusing error message and hang.\
Machine A: Agreed.</p><p>Machine B: Me too.</p><p>After I found this out simply passing <code>--mca btl_tcp_if_exclude virbr0,lo</code> to <code>mpirun</code> suppressed any contact through that odd interface and loopback and it worked perfectly.</p><h3 id=problem-four>Problem Four</h3><p>It crashes on more than four machines.</p><h3 id=problem-five>Problem Five</h3><p>It&rsquo;s about 7 times slower than running on a single one of the newer machines in the cluster.</p><h3 id=lesson-learned>Lesson Learned:</h3><p>Sometimes clusters are better left unused.</p></content><p><a href=https://jamiejquinn.com/tags/openmpi/>#Openmpi</a>
<a href=https://jamiejquinn.com/tags/code/>#Code</a>
<a href=https://jamiejquinn.com/tags/hpc/>#Hpc</a>
<a href=https://jamiejquinn.com/tags/mpi/>#Mpi</a>
<a href=https://jamiejquinn.com/tags/fortran/>#Fortran</a></p></main><footer><p><a href=mailto:jamiejquinn@jamiejquinn.com>Email</a> / <a href=https://www.github.com/jamiejquinn>Github</a> / <a href=https://www.instagram.com/jamiejquinn>Instagram</a> / <a href=https://twitter.com/jimjonquinn>Twitter</a></p></footer></body></html>