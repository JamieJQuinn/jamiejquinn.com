<!doctype html><html lang=en-GB><head><meta http-equiv=X-Clacks-Overhead content="GNU Terry Pratchett"><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" href=https://jamiejquinn.com/static/favicon.ico><title>Analysing the prevalence of continuous integration in JOSS | Jamie J Quinn</title>
<meta name=title content="Analysing the prevalence of continuous integration in JOSS"><meta name=description content="JOSS is the Journal of Open Source Software and can be considered a collection of some excellent scientific codebases. Whether they&rsquo;re exemplars of good coding practices is up for debate but I wanted to know roughly how many published papers (and associated codebases) were using specifically continuous integration of some form. Luckily I have some experience scraping web pages with python and the excellent webscraping package BeautifulSoup, so I cracked out the old tutorials and set to work."><meta name=keywords content="code,continuous-integration,webscraping,"><meta property="og:url" content="https://jamiejquinn.com/analysing-the-prevalence-of-continuous-integration-in-joss/"><meta property="og:site_name" content="Jamie J Quinn"><meta property="og:title" content="Analysing the prevalence of continuous integration in JOSS"><meta property="og:description" content="JOSS is the Journal of Open Source Software and can be considered a collection of some excellent scientific codebases. Whether they&amp;rsquo;re exemplars of good coding practices is up for debate but I wanted to know roughly how many published papers (and associated codebases) were using specifically continuous integration of some form. Luckily I have some experience scraping web pages with python and the excellent webscraping package BeautifulSoup, so I cracked out the old tutorials and set to work."><meta property="og:locale" content="en-GB"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2021-01-20T18:42:42+00:00"><meta property="article:modified_time" content="2021-01-20T18:42:42+00:00"><meta property="article:tag" content="Code"><meta property="article:tag" content="Continuous-Integration"><meta property="article:tag" content="Webscraping"><meta property="og:image" content="https://jamiejquinn.com/images/share.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jamiejquinn.com/images/share.png"><meta name=twitter:title content="Analysing the prevalence of continuous integration in JOSS"><meta name=twitter:description content="JOSS is the Journal of Open Source Software and can be considered a collection of some excellent scientific codebases. Whether they&rsquo;re exemplars of good coding practices is up for debate but I wanted to know roughly how many published papers (and associated codebases) were using specifically continuous integration of some form. Luckily I have some experience scraping web pages with python and the excellent webscraping package BeautifulSoup, so I cracked out the old tutorials and set to work."><meta itemprop=name content="Analysing the prevalence of continuous integration in JOSS"><meta itemprop=description content="JOSS is the Journal of Open Source Software and can be considered a collection of some excellent scientific codebases. Whether they&rsquo;re exemplars of good coding practices is up for debate but I wanted to know roughly how many published papers (and associated codebases) were using specifically continuous integration of some form. Luckily I have some experience scraping web pages with python and the excellent webscraping package BeautifulSoup, so I cracked out the old tutorials and set to work."><meta itemprop=datePublished content="2021-01-20T18:42:42+00:00"><meta itemprop=dateModified content="2021-01-20T18:42:42+00:00"><meta itemprop=wordCount content="1557"><meta itemprop=image content="https://jamiejquinn.com/images/share.png"><meta itemprop=keywords content="Code,Continuous-Integration,Webscraping"><meta name=referrer content="no-referrer-when-downgrade"><style>body{font-family:Verdana,sans-serif;margin:auto;padding:20px;max-width:720px;text-align:left;background-color:#fff;word-wrap:break-word;overflow-wrap:break-word;line-height:1.5;color:#444}h1,h2,h3,h4,h5,h6,strong,b{color:#222}a{text-decoration:none;color:#c50}.title{text-decoration:none;border:0}.title span{font-weight:400}nav a{margin-right:10px}textarea{width:100%;font-size:16px}input{font-size:16px}content{line-height:1.6}table{width:100%}img{max-width:100%;display:block;margin-left:auto;margin-right:auto}code{padding:2px 5px;background-color:#f2f2f2}pre code{color:#222;display:block;padding:20px;white-space:pre-wrap;font-size:14px;overflow-x:auto}div.highlight pre{background-color:initial;color:initial}div.highlight code{background-color:unset;color:unset}blockquote{border-left:1px solid #999;color:#222;padding-left:20px;font-style:italic}footer{padding:25px;text-align:center}.helptext{color:#777;font-size:small}.errorlist{color:#eba613;font-size:small}ul.blog-posts{list-style-type:none;padding:unset}ul.blog-posts li{display:flex}ul.blog-posts li span{flex:0 0 130px}ul.blog-posts li a:visited{color:#3273dc}@media(prefers-color-scheme:dark){body{background-color:#333;color:#ddd}h1,h2,h3,h4,h5,h6,strong,b{color:#eee}a{color:#8cc2dd}code{background-color:#777}pre code{color:#ddd}blockquote{color:#ccc}textarea,input{background-color:#252525;color:#ddd}.helptext{color:#aaa}}</style><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"]]}}</script></head><body><header><a href=/ class=title><h2>Jamie J Quinn / Analysing the prevalence of continuous integration in JOSS</h2></a><nav><a href=/>Home</a>
<a href=/blog/>Blog</a>
<a href=/presentations/>Presentations</a>
<a href=/projects/>Projects</a>
<a href=/publications/>Publications</a>
<a href=http://art.jamiejquinn.com>Art</a>
<a href=/jamie_quinn_cv.pdf>CV</a></nav></header><main><h1>Analysing the prevalence of continuous integration in JOSS</h1><p><i><time datetime=2021-01-20 pubdate>20 Jan, 2021</time></i></p><content><p>JOSS is the <a href=https://joss.theoj.org/>Journal of Open Source Software</a> and can be considered a collection of some excellent scientific codebases. Whether they&rsquo;re exemplars of good coding practices is up for debate but I wanted to know roughly how many published papers (and associated codebases) were using specifically continuous integration of some form. Luckily I have some experience scraping web pages with python and the excellent webscraping package BeautifulSoup, so I cracked out the <a href=https://www.crummy.com/software/BeautifulSoup/bs4/doc/>old tutorials</a> and set to work.</p><p><strong>Find the full source code in <a href=https://github.com/JamieJQuinn/joss-scraper>this Github repo</a>.</strong></p><h2 id=scraping-the-index>Scraping the index</h2><p>Webscraping is fundamentally very simple. The page is downloaded using the <code>requests</code> library:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_response</span>(page_url):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Returns response to html request on page_url&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    response <span style=color:#f92672>=</span> requests<span style=color:#f92672>.</span>get(page_url)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> response
</span></span></code></pre></div><p>Then that&rsquo;s loaded into BeautifulSoup with</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>soup <span style=color:#f92672>=</span> BeautifulSoup(response<span style=color:#f92672>.</span>content, <span style=color:#e6db74>&#39;lxml&#39;</span>)
</span></span></code></pre></div><p>which produces a tree structure which can be searched with <code>find</code> and <code>find_all</code> and moved around using e.g. <code>.link</code> or <code>.parent</code>. In my case (as we&rsquo;ll find out in the following paragraphs), I could extract all the paper URLs using</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>paper_entries <span style=color:#f92672>=</span> soup<span style=color:#f92672>.</span>find_all(<span style=color:#e6db74>&#34;entry&#34;</span>)
</span></span><span style=display:flex><span>paper_urls <span style=color:#f92672>+=</span> [entry<span style=color:#f92672>.</span>link<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;href&#39;</span>) <span style=color:#66d9ef>for</span> entry <span style=color:#f92672>in</span> paper_entries]
</span></span></code></pre></div><p>To find out how to parse a particular site, you have to get your hands dirty and start inspecting the HTML of the page you want to scrape. In my case, I wanted to know the URLs of each item in <a href="https://joss.theoj.org/papers/published?page=1">the index of JOSS</a> so I went to the page in Firefox, right-clicked on the title of an item and clicked <code>Inspect Element</code>. This brings up a useful little HTML inspector tool which allows you to figure out how to identify different elements of the page (see screenshot below).</p><p><img src=/assets/img/ci-joss/joss-inspect-element.png alt></p><p>In my case I wanted the URL of the page associated with the paper (&ldquo;nap&rdquo; in the screenshot). That&rsquo;s stored in the href of the <code>&lt;a></code> tag inside the <code>&lt;h2></code> title which is of the class &lsquo;paper-title&rsquo;. That relationship allowed me to construct a query for all matching <code>&lt;h2></code> tags, <code>paper_entries=soup.find_all('h2', class_='paper-title')</code>, and then extract the href from the <code>&lt;a></code> child. Putting that all together gave me a BeautifulSoup path which was <strong>completely incorrect</strong>.</p><p>Turns out the JOSS web server was recognising the request was coming from a non-browser and returned the atom feed instead. Nicely, BeautifulSoup can also parse those so after printing it out it was clear that each entry was kept in a <code>&lt;entry></code> tag and the link to the individual paper page stored in the <code>&lt;link></code> tag.</p><p>The URLs were then saved to a text file:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>with</span> open(<span style=color:#e6db74>&#34;ids.txt&#34;</span>, <span style=color:#e6db74>&#39;w&#39;</span>) <span style=color:#66d9ef>as</span> fp:
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> item <span style=color:#f92672>in</span> paper_urls:
</span></span><span style=display:flex><span>        fp<span style=color:#f92672>.</span>write(<span style=color:#e6db74>&#34;</span><span style=color:#e6db74>%s</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#34;</span> <span style=color:#f92672>%</span> item)
</span></span></code></pre></div><p>What I then realised was each URL was nearly identical barring a single ID at the very end. By stripping out the IDs using</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat paper_url_list.txt | awk -F. <span style=color:#e6db74>&#39;{print $5}&#39;</span> &gt; id_list.txt
</span></span></code></pre></div><p>I could reform the URL easily <em>and</em> use the ID as just that; a unique ID.</p><p>I should note that much of the above code was wrapped in a loop which iterated through the pages of the index (i.e. <code>"https://joss.theoj.org/papers/published?page="+str(i)</code>). See the Github repo linked at the top of the page for the full source code.</p><h2 id=scraping-the-metadata>Scraping the metadata</h2><p>Now I had a full index of all the paper IDs, I wanted to be able to scrape metadata from the associated URLs individually. One option was to load this list into python and loop over the list, scraping and saving the data for each paper. However, if it crashed for whatever reason, I didn&rsquo;t want to lose progress and it can be a pain to parallelise tasks like this in python (not really true, there&rsquo;s the Pool part of the multiprocessing library but it&rsquo;s still more complex than I&rsquo;d like). I realised I could use the tool <code>make</code> to solve both problems since it has inbuilt parallelisation (with the <code>-j</code> flag) and will automatically recognise when a file has been created (i.e. the output from the python script in this case) and will not run the same process again, that is it has some concept of state.</p><p>To fit into a <code>make</code> workflow I wrote a python script which takes one single paper URL, scrapes the page and spits out JSON-formatted metadata into a file so that it represented one single atomic operation. I then wrote a simple makefile which took in the list of IDs, formed the URLs and used the python script to output the associated metadata:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-make data-lang=make><span style=display:flex><span>ID_FILE<span style=color:#f92672>=</span>data/id_list.txt
</span></span><span style=display:flex><span>IDS<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>file &lt; <span style=color:#66d9ef>$(</span>ID_FILE<span style=color:#66d9ef>))</span>
</span></span><span style=display:flex><span>JSON_FILES<span style=color:#f92672>=</span><span style=color:#66d9ef>$(</span>addprefix data/,<span style=color:#66d9ef>$(</span>addsuffix .json,<span style=color:#66d9ef>$(</span>IDS<span style=color:#66d9ef>)))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>all</span><span style=color:#f92672>:</span> <span style=color:#66d9ef>$(</span>JSON_FILES<span style=color:#66d9ef>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#a6e22e>data/%.json</span><span style=color:#f92672>:</span>
</span></span><span style=display:flex><span>	python ./fetch_paper_details.py https://joss.theoj.org/papers/10.21105/joss.$* &gt; $@
</span></span></code></pre></div><p>Running <code>make -j4</code> on my 4-core laptop massively speeds up the scraping of over 1000 webpages. The python script loaded the URL, parsed it using BeautifulSoup and extracted the title from the meta <code>div</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>meta <span style=color:#f92672>=</span> soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;div&#39;</span>, class_<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;paper-meta&#39;</span>)
</span></span><span style=display:flex><span>title <span style=color:#f92672>=</span> meta<span style=color:#f92672>.</span>h1<span style=color:#f92672>.</span>string
</span></span></code></pre></div><p>the dates using the <code>dateutil</code> library</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>date_spans <span style=color:#f92672>=</span> meta<span style=color:#f92672>.</span>find_all(<span style=color:#e6db74>&#39;span&#39;</span>, class_<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;small&#39;</span>)
</span></span><span style=display:flex><span>date_strings <span style=color:#f92672>=</span> [
</span></span><span style=display:flex><span>    parse(<span style=color:#e6db74>&#34; &#34;</span><span style=color:#f92672>.</span>join(span<span style=color:#f92672>.</span>string<span style=color:#f92672>.</span>split()[<span style=color:#ae81ff>1</span>:]))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> span <span style=color:#f92672>in</span> date_spans
</span></span><span style=display:flex><span>]
</span></span></code></pre></div><p>extracted the tags associated with the languages used in the paper</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>lang_spans <span style=color:#f92672>=</span> meta<span style=color:#f92672>.</span>find_all(<span style=color:#e6db74>&#39;span&#39;</span>, class_<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;badge-lang&#39;</span>)
</span></span><span style=display:flex><span>lang_tags <span style=color:#f92672>=</span> [span<span style=color:#f92672>.</span>string <span style=color:#66d9ef>for</span> span <span style=color:#f92672>in</span> lang_spans]
</span></span></code></pre></div><p>extracted the tags describing the field in which the paper sits from the sidebar of the page</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>sidebar <span style=color:#f92672>=</span> soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;div&#39;</span>, class_<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;paper-sidebar&#39;</span>)
</span></span><span style=display:flex><span>tag_spans <span style=color:#f92672>=</span> sidebar<span style=color:#f92672>.</span>find_all(<span style=color:#e6db74>&#39;span&#39;</span>, class_<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;badge-lang&#39;</span>)
</span></span><span style=display:flex><span>field_tags <span style=color:#f92672>=</span> [span<span style=color:#f92672>.</span>string <span style=color:#66d9ef>for</span> span <span style=color:#f92672>in</span> tag_spans]
</span></span></code></pre></div><p>and finally extracted the repository URL which just happens to be the first link which matches the search criteria</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>repo_url <span style=color:#f92672>=</span> soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;a&#39;</span>, class_<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;paper-btn&#39;</span>)<span style=color:#f92672>.</span>get(<span style=color:#e6db74>&#39;href&#39;</span>)
</span></span></code></pre></div><p>This last line is quite fragile. I wouldn&rsquo;t necessarily depend on it to work if the page changed in future, but it works now and I only want the data once! All this was stored in a <code>dict</code> then saved as a JSON file</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>data <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;title&#34;</span>: title,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;submission_date&#34;</span>: str(date_strings[<span style=color:#ae81ff>0</span>]),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;publish_date&#34;</span>: str(date_strings[<span style=color:#ae81ff>1</span>]),
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;lang_tags&#34;</span>: lang_tags,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;field_tags&#34;</span>: field_tags,
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;repo_url&#34;</span>: repo_url
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=determining-the-ci-platform>Determining the CI platform</h2><p>Now that I had a directory filled with JSON files each describing the metadata of one single paper, including the URL of the codebase repository I wondered how many papers actually stored their code in Github. Using <code>cat *.json | sed 's/, /\n/g' | grep repo_url | awk -F'/' '{print $3}' | sort | uniq -c</code> and some manual cleaning produced</p><pre tabindex=0><code>   17 bitbucket.org
    1 c4science.ch
    1 doi.org
    1 framagit.org
 1086 github.com
    1 git.iws.uni-stuttgart.de
   22 gitlab.com
    1 gitlab.gwdg.de
    3 gitlab.inria.fr
    1 gricad-gitlab.univ-grenoble-alpes.fr
    1 marcoalopez.github.io
    1 mutabit.com
    1 savannah.nongnu.org
    1 sourceforge.net
    3 ts-gitlab.iup.uni-heidelberg.de
    1 www.idpoisson.fr
</code></pre><p>which strongly shows Github is the dominant platform for JOSS submissions and means I only have to write a scraper targetting one platform.</p><p>I looked at the source for a Github repository&rsquo;s homepage and discovered a file in the repo appears in the webpage as a simple link with the title matching the filename in the repo, so I can search for, say, a Travis config file <code>.travis.yml</code> using</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;a&#39;</span>, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;.travis.yml&#39;</span>)
</span></span></code></pre></div><p>After looking through around 30 of the most recent JOSS published papers, I could only find 3 continuous integration platforms in obvious use: Circle CI, Travis and Github Actions. These were signalled using</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>    <span style=color:#66d9ef>if</span> soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;a&#39;</span>, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;.circleci&#39;</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Is there a folder called .circleci?</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;circle-ci&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;span&#39;</span>, string<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;.github/&#39;</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># GH renders a github actions folder as a &lt;span&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;github-actions&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;a&#39;</span>, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;.github&#39;</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># GH renders a .github folder as an &lt;a&gt;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;github-folder&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>elif</span> soup<span style=color:#f92672>.</span>find(<span style=color:#e6db74>&#39;a&#39;</span>, title<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;.travis.yml&#39;</span>):
</span></span><span style=display:flex><span>        <span style=color:#75715e># Is there a file called .travis.yml?</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#e6db74>&#34;travis&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#66d9ef>False</span>
</span></span></code></pre></div><p>with an additional option in case the repo wasn&rsquo;t actually stored in Github. Note, where <code>.github</code> was stored in a <code>&lt;span></code> tag, that represented the situation where the <code>.github</code> folder only contained a <code>workflows</code> folder, indicating the project was using Github Actions. Where it was stored in a <code>&lt;a></code> tag instead, that represented the situation where other files were stored in the <code>.github</code> folder. For the sake of simplicity, I&rsquo;m just counting those as <em>also</em> using Github Actions, even though that may not be true.</p><p>Again I used the <code>make</code> workflow to parallelise this whole process and the result was one file per paper (named <code>[ID]_github.txt</code>) storing the result of the above analysis.</p><h2 id=results>Results</h2><p>Using <code>cat *_github.txt | sort | uniq -c</code> gave a quick count of the number of papers using each platform and the results are stark:
<img src=/assets/img/ci-joss/occurances.png alt></p><p>Or in raw numbers:</p><pre tabindex=0><code> 13 circle-ci
 50 github-actions
 72 github-folder // Could be github actions
110 travis

845 False // Couldn&#39;t find obivous CI
 55 not-github
</code></pre><p>What we can draw from this is that the majority of projects do not use an <em>obvious</em> CI platform. Of course, some JOSS codebases are not stored in Github and some may be using webhooks or other tools to run alternative CI workflows like with Jenkins. Still, the sheer volume of codebases without any obvious CI setup is at least indicative of poor adoption of CI, even in codebases which have been peer-reviewed and published in an academic journal.</p><p>Thank you for reading this rather long and rambling recount of my recent obsession with continuous integration adoption levels. I hope you take away something from this, whether the use of make as a great tool for simple parallelisation, or BeautifulSoup as a fantastic webscraper, or some motivation to setup CI in your project before you become a statistic in my project. This entire analysis has many caveats and only rudimentary discovery of the CI platform attached to a codebase. If you have any ideas for improving this, please get in touch via <a href=https://twitter.com/jimjonquinn>twitter</a> or by <a href=mailto:jamiejquinn@jamiejquinn.com>email</a>.</p><p>PS. If you&rsquo;re associated with JOSS, if there&rsquo;s already an API then I&rsquo;m sorry for thrashing your servers, but if there&rsquo;s not&mldr; should there be?</p></content><p><a href=https://jamiejquinn.com/tags/code/>#Code</a>
<a href=https://jamiejquinn.com/tags/continuous-integration/>#Continuous-Integration</a>
<a href=https://jamiejquinn.com/tags/webscraping/>#Webscraping</a></p></main><footer><p><a href=mailto:jamiejquinn@jamiejquinn.com>Email</a> / <a href=https://www.github.com/jamiejquinn>Github</a> / <a href=https://www.instagram.com/jamiejquinn>Instagram</a> / <a href=https://twitter.com/jimjonquinn>Twitter</a></p></footer></body></html>